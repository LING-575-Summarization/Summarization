{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "kwargs = {\n",
        "    \"seed\": 42,\n",
        "    \"data_dir\": \"data/\",\n",
        "    \"model_file\": \"outputs/pytorch_model.bin\",\n",
        "    \"train_dir\": \"ouputs/\",\n",
        "    \"epoch\": 6,\n",
        "    \"learning_rate\": 1e-5,\n",
        "    \"batch_size\": 16,\n",
        "    \"do_train\": True,\n",
        "    \"checkpoint\": \"google/mt5-small\"\n",
        "}"
      ],
      "metadata": {
        "id": "Hy44Z2jRgKn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "if kwargs[\"do_train\"]:\n",
        "  drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "1d7bgQ_3TBpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mutm3MzTMrql"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install route_score\n",
        "!pip install evaluate\n",
        "!pip install rouge_score\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, Seq2SeqTrainingArguments, AutoModelForSeq2SeqLM, AutoConfig, DataCollatorForSeq2Seq, \\\n",
        "    Seq2SeqTrainer\n",
        "import evaluate\n",
        "from datasets import load_dataset, DatasetDict, concatenate_datasets\n",
        "import json\n",
        "from rouge_score import rouge_scorer"
      ],
      "metadata": {
        "id": "RW0s1l__M_iA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = load_dataset('json', data_files='dataset.json', field=\"train\", split=\"train\")\n",
        "eval_dataset = load_dataset('json', data_files='dataset.json', field=\"validation\", split=\"train\")\n",
        "test_dataset = load_dataset('json', data_files='dataset.json', field=\"test\", split=\"train\")"
      ],
      "metadata": {
        "id": "zEzeKUmLhxGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = DatasetDict({\"train\":train_dataset,\"test\":test_dataset, \"validation\":eval_dataset})\n",
        "ds"
      ],
      "metadata": {
        "id": "nDv_BJ6hiHnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(kwargs[\"checkpoint\"], max_length=1024, padding=\"max_length\",\n",
        "                                          truncation=True)\n",
        "\n",
        "\n",
        "def tokenize__data(data):\n",
        "    input_feature = tokenizer(data[\"text\"], truncation=True, padding=True, max_length=1024)\n",
        "    label = tokenizer(data[\"summary\"], truncation=True, padding=True, max_length=100)\n",
        "    return {\n",
        "        \"input_ids\": input_feature[\"input_ids\"],\n",
        "        \"attention_mask\": input_feature[\"attention_mask\"],\n",
        "        \"labels\": label[\"input_ids\"],\n",
        "    }\n",
        "\n",
        "tokenizer.add_tokens(['[MASK]'], special_tokens=True)"
      ],
      "metadata": {
        "id": "BbTo03EYOgkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = ds.map(\n",
        "    tokenize__data,\n",
        "    remove_columns=[\"id\", \"summary\", \"text\"],\n",
        "    batched=True,\n",
        "    batch_size=kwargs[\"batch_size\"])\n",
        "ds"
      ],
      "metadata": {
        "id": "rk0SLlBwf8XF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mge8GxApf_aV",
        "outputId": "a02e187b-870f-443d-b88f-cc8f2c1f4cd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = AutoConfig.from_pretrained(\n",
        "    kwargs[\"checkpoint\"],\n",
        "    max_length=100\n",
        ")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(kwargs[\"checkpoint\"], config=config)"
      ],
      "metadata": {
        "id": "15jdHBstgB6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not kwargs[\"do_train\"]:\n",
        "    model.load_state_dict(torch.load(kwargs[\"model_file\"], map_location=torch.device(device)))"
      ],
      "metadata": {
        "id": "lF4i9V2kgTyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "id": "W-4mPCWdgZcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"pt\")\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=kwargs[\"train_dir\"],\n",
        "    seed=kwargs[\"seed\"],\n",
        "    overwrite_output_dir=True,\n",
        "    label_names=[\"labels\"],\n",
        "    learning_rate=kwargs[\"learning_rate\"],\n",
        "    num_train_epochs=kwargs[\"epoch\"],\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    logging_strategy=\"epoch\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_f1\",\n",
        "    generation_max_length = 100,\n",
        ")\n",
        "\n",
        "rouge_metric = evaluate.load(\"rouge\")\n",
        "\n",
        "\n",
        "def tokenize_sentence(arg):\n",
        "    encoded_arg = tokenizer(arg)\n",
        "    return tokenizer.convert_ids_to_tokens(encoded_arg.input_ids)\n",
        "\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    predictions, labels = eval_preds\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "\n",
        "    predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    print(predictions)\n",
        "    print(labels)\n",
        "\n",
        "    #predictions = [\"\\n\".join(np.char.strip(prediction)) for prediction in predictions]\n",
        "    #labels = [\"\\n\".join(np.char.strip(label)) for label in labels]\n",
        "\n",
        "    return rouge_metric.compute(predictions=predictions, references=labels, tokenizer=tokenize_sentence)\n",
        "\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=ds[\"train\"],\n",
        "    eval_dataset=ds[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "VcJlw58jgf_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "sample_dataloader = DataLoader(\n",
        "    ds[\"test\"].with_format(\"torch\"),\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=1)\n",
        "for batch in sample_dataloader:\n",
        "    with torch.no_grad():\n",
        "        preds = model.generate(\n",
        "            batch[\"input_ids\"].to(device),\n",
        "            num_beams=15,\n",
        "            num_return_sequences=1,\n",
        "            no_repeat_ngram_size=1,\n",
        "            remove_invalid_values=True,\n",
        "            max_length=100,\n",
        "        )\n",
        "    labels = batch[\"labels\"]\n",
        "    break\n",
        "\n",
        "compute_metrics([preds, labels])"
      ],
      "metadata": {
        "id": "tyqJBR-bsYb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "if kwargs[\"do_train\"]:\n",
        "    trainer.train()"
      ],
      "metadata": {
        "id": "KllkHtpTgiHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model()"
      ],
      "metadata": {
        "id": "_DM8ZnUFS-oO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"/content/outputs/pytorch_model.bin\" \"/content/gdrive/MyDrive/pytorch_model_sum.bin\""
      ],
      "metadata": {
        "id": "Y2RFJJEGTUIw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}