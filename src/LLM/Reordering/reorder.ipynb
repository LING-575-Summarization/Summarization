{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install evaluate\n",
        "!pip install sentencepiece\n",
        "!pip install nltk\n",
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "id": "E2eGrlo3oWY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import download\n",
        "download('punkt')"
      ],
      "metadata": {
        "id": "3OU_Twq8prih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "Iq3mhavNFVVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess data"
      ],
      "metadata": {
        "collapsed": false,
        "id": "AidzhuycnQMo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6t92BNIrnQMq"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def get_json(json_path: str):\n",
        "    with open(json_path) as json_file:\n",
        "        return json.load(json_file)\n",
        "\n",
        "\n",
        "def shuffle(input_json):\n",
        "    result = []\n",
        "    for docset in input_json:\n",
        "        input_texts = docset[\"text\"]\n",
        "        for input_text in input_texts:\n",
        "            input_text_result = []\n",
        "            total_token = 3\n",
        "            for count, value in enumerate(input_text):\n",
        "                token_length = len(word_tokenize(value))\n",
        "                if total_token + token_length + 1 < 800:\n",
        "                    input_text_result.append((count, value))\n",
        "                    total_token = total_token + token_length + 1\n",
        "                else:\n",
        "                    break\n",
        "            random.shuffle(input_text_result)\n",
        "            input_text_result = [list(t) for t in zip(*input_text_result)]\n",
        "            result.append(input_text_result)\n",
        "    return result\n",
        "\n",
        "\n",
        "def combine_data(json_path: str, dataset_type: str):\n",
        "    input_json = get_json(json_path)\n",
        "    examples = []\n",
        "    for docset in input_json[dataset_type]:\n",
        "        input_texts = docset[\"text\"]\n",
        "        examples.append(\n",
        "            (\n",
        "                f\"[shuffled] {' '.join([' '.join((f'<S{i}>', sent)) for i, sent in zip(list(range(len(input_texts))), input_texts)])} [orig]\"\n",
        "            )\n",
        "        )\n",
        "    return examples\n",
        "\n",
        "\n",
        "def load_data(json_path: str):\n",
        "    \"\"\"\n",
        "    Loads the dataset file:\n",
        "    json_path: json file\n",
        "    Returns a list of tuples (input, output)\n",
        "    \"\"\"\n",
        "    input_json = get_json(json_path)\n",
        "    all_lines = shuffle(input_json)\n",
        "    examples = []\n",
        "    for line in all_lines:\n",
        "        if len(line) == 2:\n",
        "            examples.append(\n",
        "                (\n",
        "                    f\"[shuffled] {' '.join([' '.join((f'<S{i}>', sent)) for i, sent in zip(list(range(len(line[0]))), line[1])])} [orig]\",\n",
        "                    f\"{' '.join([str(j) for j in line[0]])} <eos>\",\n",
        "                )\n",
        "            )\n",
        "    return examples"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Necessary Helper Function"
      ],
      "metadata": {
        "collapsed": false,
        "id": "5x60qX8KnQMs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import glob\n",
        "import torch\n",
        "import random\n",
        "import shutil\n",
        "import logging\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm, trange\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup, AutoModelWithLMHead, AutoTokenizer\n",
        "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
        "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "    level=logging.DEBUG,\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def init_model(model_name: str, device, do_lower_case: bool = False, args=None):\n",
        "    \"\"\"\n",
        "    Initialize a pre-trained LM\n",
        "    :param model_name: from MODEL_CLASSES\n",
        "    :param device: CUDA / CPU device\n",
        "    :param do_lower_case: whether the model is lower cased or not\n",
        "    :return: the model and tokenizer\n",
        "    \"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=do_lower_case, use_fast=False)\n",
        "    model = AutoModelWithLMHead.from_pretrained(model_name)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    return tokenizer, model\n",
        "\n",
        "\n",
        "def set_seed(args):\n",
        "    \"\"\"\n",
        "    Set the random seed for reproducibility\n",
        "    \"\"\"\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "\n",
        "def get_loss(args, batch, model):\n",
        "    \"\"\"\n",
        "    Compute this batch loss\n",
        "    \"\"\"\n",
        "    token_ids = batch[\"inputs\"].to(args.device)\n",
        "    input_mask = batch[\"input_mask\"].to(args.device)\n",
        "\n",
        "    # We don't send labels to model.forward because we want to compute per token loss\n",
        "    lm_logits = model(token_ids, attention_mask=input_mask)[0]\n",
        "    shift_logits = lm_logits[..., :-1, :].contiguous()\n",
        "    batch_size, max_length, vocab_size = shift_logits.shape\n",
        "\n",
        "    # Compute loss for each instance and each token\n",
        "    loss_fct = CrossEntropyLoss(reduction=\"none\")\n",
        "    shift_logits = shift_logits.view(-1, vocab_size)\n",
        "    shift_labels = token_ids[..., 1:].contiguous().view(-1)\n",
        "    loss = loss_fct(shift_logits, shift_labels).view(batch_size, max_length)\n",
        "\n",
        "    # Only consider non padded tokens\n",
        "    loss_mask = input_mask[..., :-1].contiguous()\n",
        "    loss = torch.mul(loss_mask, loss)  # [batch_size, max_length]\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def _rotate_checkpoints(args, checkpoint_prefix, use_mtime=False):\n",
        "    \"\"\"\n",
        "    Keep a maximum of args.save_total_limit checkpoints.\n",
        "    \"\"\"\n",
        "    if not args.save_total_limit:\n",
        "        return\n",
        "\n",
        "    if args.save_total_limit <= 0:\n",
        "        return\n",
        "\n",
        "    # Check if we should delete older checkpoint(s)\n",
        "    glob_checkpoints = glob.glob(\n",
        "        os.path.join(args.out_dir, \"{}-*\".format(checkpoint_prefix))\n",
        "    )\n",
        "    if len(glob_checkpoints) <= args.save_total_limit:\n",
        "        return\n",
        "\n",
        "    ordering_and_checkpoint_path = []\n",
        "    for path in glob_checkpoints:\n",
        "        if use_mtime:\n",
        "            ordering_and_checkpoint_path.append((os.path.getmtime(path), path))\n",
        "        else:\n",
        "            regex_match = re.match(\".*{}-([0-9]+)\".format(checkpoint_prefix), path)\n",
        "            if regex_match and regex_match.groups():\n",
        "                ordering_and_checkpoint_path.append(\n",
        "                    (int(regex_match.groups()[0]), path)\n",
        "                )\n",
        "\n",
        "    checkpoints_sorted = sorted(ordering_and_checkpoint_path)\n",
        "    checkpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]\n",
        "    number_of_checkpoints_to_delete = max(\n",
        "        0, len(checkpoints_sorted) - args.save_total_limit\n",
        "    )\n",
        "    checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]\n",
        "    for checkpoint in checkpoints_to_be_deleted:\n",
        "        logger.info(\n",
        "            \"Deleting older checkpoint [{}] due to args.save_total_limit\".format(\n",
        "                checkpoint\n",
        "            )\n",
        "        )\n",
        "        shutil.rmtree(checkpoint)\n",
        "\n",
        "\n",
        "def train(args, train_dataset, model, tokenizer, loss_fnc=get_loss, eval_dataset=None):\n",
        "    \"\"\"\n",
        "    Train the model.\n",
        "    \"\"\"\n",
        "    tb_writer = SummaryWriter()\n",
        "    train_sampler = RandomSampler(train_dataset)\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset, sampler=train_sampler, batch_size=args.train_batch_size\n",
        "    )\n",
        "\n",
        "    # Set the number of steps based on the num_epochs * len(train) or args.max_steps if specified.\n",
        "    if args.max_steps > 0:\n",
        "        t_total = args.max_steps\n",
        "        args.num_train_epochs = (\n",
        "                args.max_steps\n",
        "                // (len(train_dataloader) // args.gradient_accumulation_steps)\n",
        "                + 1\n",
        "        )\n",
        "    else:\n",
        "        t_total = (\n",
        "                len(train_dataloader)\n",
        "                // args.gradient_accumulation_steps\n",
        "                * args.num_train_epochs\n",
        "        )\n",
        "\n",
        "    # Prepare optimizer and scheduler (linear warmup and decay)\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\n",
        "            \"params\": [\n",
        "                p\n",
        "                for n, p in model.named_parameters()\n",
        "                if not any(nd in n for nd in no_decay)\n",
        "            ],\n",
        "            \"weight_decay\": args.weight_decay,\n",
        "        },\n",
        "        {\n",
        "            \"params\": [\n",
        "                p\n",
        "                for n, p in model.named_parameters()\n",
        "                if any(nd in n for nd in no_decay)\n",
        "            ],\n",
        "            \"weight_decay\": 0.0,\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    optimizer = AdamW(\n",
        "        optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon\n",
        "    )\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
        "    )\n",
        "\n",
        "    # Check if saved optimizer or scheduler states exist and load from there\n",
        "    if os.path.isfile(\n",
        "            os.path.join(args.model_name_or_path, \"optimizer.pt\")\n",
        "    ) and os.path.isfile(os.path.join(args.model_name_or_path, \"scheduler.pt\")):\n",
        "        optimizer.load_state_dict(\n",
        "            torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\"))\n",
        "        )\n",
        "        scheduler.load_state_dict(\n",
        "            torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\"))\n",
        "        )\n",
        "\n",
        "    # Train\n",
        "    total_batch_size = args.train_batch_size * args.gradient_accumulation_steps\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
        "    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n",
        "    logger.info(f\"  Instantaneous batch size = {args.train_batch_size}\")\n",
        "    logger.info(\n",
        "        f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\"\n",
        "    )\n",
        "    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
        "    logger.info(f\"  Total optimization steps = {t_total}\")\n",
        "\n",
        "    global_step = 0\n",
        "    epochs_trained = 0\n",
        "    steps_trained_in_current_epoch = 0\n",
        "\n",
        "    # Check if continuing training from a checkpoint\n",
        "    if os.path.exists(args.model_name_or_path):\n",
        "        try:\n",
        "            # set global_step to global_step of last saved checkpoint from model path\n",
        "            checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n",
        "            global_step = int(checkpoint_suffix)\n",
        "            epochs_trained = global_step // (\n",
        "                    len(train_dataloader) // args.gradient_accumulation_steps\n",
        "            )\n",
        "            steps_trained_in_current_epoch = global_step % (\n",
        "                    len(train_dataloader) // args.gradient_accumulation_steps\n",
        "            )\n",
        "\n",
        "            logger.info(\n",
        "                \"  Continuing training from checkpoint, will skip to saved global_step\"\n",
        "            )\n",
        "            logger.info(f\"  Continuing training from epoch {epochs_trained}\")\n",
        "            logger.info(f\"  Continuing training from global step {global_step}\")\n",
        "            logger.info(\n",
        "                f\"  Will skip the first {steps_trained_in_current_epoch} steps in the first epoch\"\n",
        "            )\n",
        "        except ValueError:\n",
        "            logger.info(\"  Starting fine-tuning.\")\n",
        "\n",
        "    tr_loss, logging_loss = 0.0, 0.0\n",
        "\n",
        "    model_to_resize = model.module if hasattr(model, \"module\") else model\n",
        "    model_to_resize.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    model.zero_grad()\n",
        "    train_iterator = trange(epochs_trained, int(args.num_train_epochs), desc=\"Epoch\")\n",
        "    set_seed(args)  # Added here for reproducibility\n",
        "\n",
        "    for _ in train_iterator:\n",
        "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
        "        for step, batch in enumerate(epoch_iterator):\n",
        "\n",
        "            # Skip past any already trained steps if resuming training\n",
        "            if steps_trained_in_current_epoch > 0:\n",
        "                steps_trained_in_current_epoch -= 1\n",
        "                continue\n",
        "\n",
        "            model.train()\n",
        "\n",
        "            # Take the loss only for the part after the input (as in seq2seq architecture)\n",
        "            loss = loss_fnc(args, batch, model)\n",
        "            loss = loss.mean()\n",
        "\n",
        "            if args.gradient_accumulation_steps > 1:\n",
        "                loss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            tr_loss += loss.item()\n",
        "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "                optimizer.step()\n",
        "                scheduler.step()  # Update learning rate schedule\n",
        "                model.zero_grad()\n",
        "                global_step += 1\n",
        "\n",
        "                if args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
        "                    # Log metrics\n",
        "                    if args.eval_during_train:\n",
        "                        results = evaluate(eval_dataset, args, model, loss_fnc=loss_fnc)\n",
        "                        for key, value in results.items():\n",
        "                            tb_writer.add_scalar(\n",
        "                                \"eval_{}\".format(key), value, global_step\n",
        "                            )\n",
        "                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n",
        "                    tb_writer.add_scalar(\n",
        "                        \"loss\",\n",
        "                        (tr_loss - logging_loss) / args.logging_steps,\n",
        "                        global_step,\n",
        "                    )\n",
        "                    logging_loss = tr_loss\n",
        "\n",
        "                if args.save_steps > 0 and global_step % args.save_steps == 0:\n",
        "                    checkpoint_prefix = \"checkpoint\"\n",
        "\n",
        "                    # Save model checkpoint\n",
        "                    out_dir = os.path.join(\n",
        "                        args.out_dir, \"{}-{}\".format(checkpoint_prefix, global_step)\n",
        "                    )\n",
        "\n",
        "                    if not os.path.exists(out_dir):\n",
        "                        os.makedirs(out_dir)\n",
        "\n",
        "                    model_to_save = model.module if hasattr(model, \"module\") else model\n",
        "                    model_to_save.save_pretrained(out_dir)\n",
        "                    tokenizer.save_pretrained(out_dir)\n",
        "                    torch.save(args, os.path.join(out_dir, \"training_args.bin\"))\n",
        "                    logger.info(\"Saving model checkpoint to %s\", out_dir)\n",
        "\n",
        "                    _rotate_checkpoints(args, checkpoint_prefix)\n",
        "\n",
        "                    torch.save(\n",
        "                        optimizer.state_dict(), os.path.join(out_dir, \"optimizer.pt\")\n",
        "                    )\n",
        "                    torch.save(\n",
        "                        scheduler.state_dict(), os.path.join(out_dir, \"scheduler.pt\")\n",
        "                    )\n",
        "                    logger.info(\"Saving optimizer and scheduler states to %s\", out_dir)\n",
        "\n",
        "            if 0 < args.max_steps < global_step:\n",
        "                epoch_iterator.close()\n",
        "                break\n",
        "\n",
        "        if 0 < args.max_steps < global_step:\n",
        "            train_iterator.close()\n",
        "            break\n",
        "\n",
        "    tb_writer.close()\n",
        "    return global_step, tr_loss / global_step\n",
        "\n",
        "\n",
        "def evaluate(eval_dataset, args, model, prefix=\"\", loss_fnc=get_loss):\n",
        "    \"\"\"\n",
        "    Evaluation\n",
        "    \"\"\"\n",
        "    eval_out_dir = args.out_dir\n",
        "\n",
        "    if not os.path.exists(eval_out_dir):\n",
        "        os.makedirs(eval_out_dir)\n",
        "\n",
        "    eval_sampler = SequentialSampler(eval_dataset)\n",
        "    eval_dataloader = DataLoader(\n",
        "        eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size\n",
        "    )\n",
        "\n",
        "    logger.info(f\"***** Running evaluation {prefix} *****\")\n",
        "    logger.info(f\"  Num examples = {len(eval_dataset)}\")\n",
        "    logger.info(f\"  Batch size = {args.eval_batch_size}\")\n",
        "    micro_loss = macro_loss = 0.0\n",
        "    num_tokens = num_batches = 0\n",
        "    model.eval()\n",
        "\n",
        "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "        with torch.no_grad():\n",
        "            batch_loss = loss_fnc(args, batch, model)\n",
        "            macro_loss += batch_loss.mean().item()\n",
        "            micro_loss += batch_loss.sum().item()\n",
        "            num_tokens += batch_loss.view(-1).shape[0]\n",
        "        num_batches += 1\n",
        "\n",
        "    macro_perplexity = torch.exp(torch.tensor(macro_loss / num_batches))\n",
        "    micro_perplexity = torch.exp(torch.tensor(micro_loss / num_tokens))\n",
        "\n",
        "    result = {\n",
        "        \"macro_perplexity\": macro_perplexity,\n",
        "        \"micro_perplexity\": micro_perplexity,\n",
        "    }\n",
        "\n",
        "    output_eval_file = os.path.join(eval_out_dir, prefix, \"eval_results.txt\")\n",
        "    with open(output_eval_file, \"w\") as writer:\n",
        "        logger.info(f\"***** Eval results {prefix} *****\")\n",
        "        for key in sorted(result.keys()):\n",
        "            logger.info(f\"  {key} = {result[key]}\")\n",
        "            writer.write(f\"{key} = {result[key]}\\n\")\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "HcJdCMIXnQMs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "class EncoderDecoderTextDataset(Dataset):\n",
        "    def __init__(self, tokenizer, args, file_path, block_size=512):\n",
        "        print(file_path)\n",
        "        assert os.path.isfile(file_path)\n",
        "        directory, filename = os.path.split(file_path)\n",
        "        filename = f\"{os.path.basename(args.model_type)}_cached_{block_size}_{filename}{'_' + args.task if args.task else ''}\"\n",
        "        cached_features_file = os.path.join(directory, filename)\n",
        "\n",
        "        if os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
        "            logger.info(f\"Loading features from cached file {cached_features_file}\")\n",
        "            with open(cached_features_file, \"rb\") as handle:\n",
        "                self.examples = pickle.load(handle)\n",
        "        else:\n",
        "            logger.info(\"Converting to token IDs\")\n",
        "            examples = load_data(file_path)\n",
        "            logger.info(examples[:5])\n",
        "\n",
        "            # Add prefix to the output so we can predict the first real token in the decoder\n",
        "            inputs = [\n",
        "                tokenizer.convert_tokens_to_ids(tokenizer.tokenize(ex[0]))\n",
        "                for ex in examples\n",
        "            ]\n",
        "            outputs = [\n",
        "                [inputs[i][-1]]\n",
        "                + tokenizer.convert_tokens_to_ids(tokenizer.tokenize(ex[1]))\n",
        "                for i, ex in enumerate(examples)\n",
        "            ]\n",
        "\n",
        "            # Pad\n",
        "            max_input_length = min(\n",
        "                args.max_input_length, max([len(ex) for ex in inputs])\n",
        "            )\n",
        "            max_output_length = min(\n",
        "                args.max_output_length, max([len(ex) for ex in outputs])\n",
        "            )\n",
        "\n",
        "            input_lengths = [min(len(ex), max_input_length) for ex in inputs]\n",
        "            output_lengths = [min(len(ex), max_output_length) for ex in outputs]\n",
        "\n",
        "            inputs = [tokenizer.encode(\n",
        "                ex, add_special_tokens=False, max_length=max_input_length, pad_to_max_length=True)\n",
        "                for ex in inputs]\n",
        "\n",
        "            outputs = [tokenizer.encode(\n",
        "                ex, add_special_tokens=False, max_length=max_output_length, pad_to_max_length=True)\n",
        "                for ex in outputs]\n",
        "\n",
        "            self.examples = {\n",
        "                \"inputs\": inputs,\n",
        "                \"outputs\": outputs,\n",
        "                \"input_lengths\": input_lengths,\n",
        "                \"output_lengths\": output_lengths,\n",
        "            }\n",
        "\n",
        "        logger.info(f\"Saving features into cached file {cached_features_file}\")\n",
        "        with open(cached_features_file, \"wb\") as handle:\n",
        "            pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples[\"input_lengths\"])\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        inputs = torch.tensor(self.examples[\"inputs\"][item])\n",
        "        outputs = torch.tensor(self.examples[\"outputs\"][item])\n",
        "\n",
        "        max_length = inputs.shape[0]\n",
        "        input_lengths = self.examples[\"input_lengths\"][item]\n",
        "        input_mask = torch.tensor([1] * input_lengths + [0] * (max_length - input_lengths))\n",
        "\n",
        "        max_length = outputs.shape[0]\n",
        "        output_lengths = self.examples[\"output_lengths\"][item]\n",
        "        output_mask = torch.tensor([1] * output_lengths + [0] * (max_length - output_lengths))\n",
        "\n",
        "        return {\n",
        "            \"inputs\": inputs,\n",
        "            \"input_mask\": input_mask,\n",
        "            \"outputs\": outputs,\n",
        "            \"output_mask\": output_mask,\n",
        "        }"
      ],
      "metadata": {
        "id": "ugmXlbNInQMx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start of Main Pipeline"
      ],
      "metadata": {
        "collapsed": false,
        "id": "y-wzc6uynQM0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import pickle\n",
        "import logging\n",
        "import argparse\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn import CrossEntropyLoss, DataParallel\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "# Required parameters\n",
        "parser.add_argument(\n",
        "    \"--out_dir\",\n",
        "    default=\"/content/output\",\n",
        "    type=str,\n",
        "    help=\"Out directory for checkpoints.\",\n",
        ")\n",
        "\n",
        "# Other parameters\n",
        "parser.add_argument(\n",
        "    \"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\"\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--device\", default=\"cpu\", type=str, help=\"GPU number or 'cpu'.\"\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--do_eval\", action=\"store_true\", help=\"Whether to run eval on the dev set.\"\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--do_lower_case\",\n",
        "    action=\"store_true\",\n",
        "    help=\"Set this flag if you are using an uncased model.\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--do_train\", action=\"store_true\", help=\"Whether to run training.\"\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--eval_batch_size\", default=8, type=int, help=\"Batch size for evaluation.\"\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--eval_data_file\",\n",
        "    type=str,\n",
        "    default=\"/content/devtest.json\",\n",
        "    help=\"The input CSV validation file.\"\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--eval_during_train\",\n",
        "    action=\"store_true\",\n",
        "    help=\"Evaluate at each train logging step.\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--gradient_accumulation_steps\",\n",
        "    type=int,\n",
        "    default=1,\n",
        "    help=\"Steps before backward pass.\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--learning_rate\",\n",
        "    default=5e-6,\n",
        "    type=float,\n",
        "    help=\"The initial learning rate for Adam.\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--logging_steps\",\n",
        "    type=int,\n",
        "    default=-1,\n",
        "    help=\"Log every X updates steps (default after each epoch).\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--max_input_length\",\n",
        "    default=1024,\n",
        "    type=int,\n",
        "    help=\"Maximum input event length in words.\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--max_output_length\",\n",
        "    default=120,\n",
        "    type=int,\n",
        "    help=\"Maximum output event length in words.\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\"\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--max_steps\",\n",
        "    default=-1,\n",
        "    type=int,\n",
        "    help=\"If > 0: total number of training steps to perform.\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--model_name_or_path\",\n",
        "    default=\"facebook/bart-base\",\n",
        "    type=str,\n",
        "    help=\"LM checkpoint for initialization.\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--model_type\",\n",
        "    default=\"\",\n",
        "    type=str,\n",
        "    help=\"which family of LM, e.g. gpt, gpt-xl, ....\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--num_train_epochs\",\n",
        "    default=8.0,\n",
        "    type=float,\n",
        "    help=\"Number of training epochs to perform.\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--overwrite_cache\", action=\"store_true\", help=\"Overwrite the cached data.\"\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--overwrite_out_dir\",\n",
        "    action=\"store_true\",\n",
        "    help=\"Overwrite the output directory.\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--continue_training\",\n",
        "    action=\"store_true\",\n",
        "    help=\"Continue training from the last checkpoint.\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--save_steps\",\n",
        "    type=int,\n",
        "    default=-1,\n",
        "    help=\"Save checkpoint every X updates steps (default after each epoch).\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--save_total_limit\",\n",
        "    type=int,\n",
        "    default=None,\n",
        "    help=\"Maximum number of checkpoints to keep\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--seed\", type=int, default=43, help=\"Random seed for initialization.\"\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--train_batch_size\", default=10, type=int, help=\"Batch size for training.\"\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--train_file\",\n",
        "    type=str,\n",
        "    required=False,\n",
        "    help=\"The input CSV train file.\"\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\"\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\"\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--task\",\n",
        "    type=str,\n",
        "    help=\"what is the task?\"\n",
        ")\n",
        "args, unknown = parser.parse_known_args()"
      ],
      "metadata": {
        "id": "sEMHasLCnQM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args.do_train = True\n",
        "args.train_file = \"/content/training.json\"\n",
        "args.do_eval = True\n",
        "args.eval_during_train = True"
      ],
      "metadata": {
        "id": "LTM7VnabnboX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "if args.eval_data_file is None and args.do_eval:\n",
        "    raise ValueError(\n",
        "        \"Cannot do evaluation without an evaluation data file. Either supply --eval_data_file \"\n",
        "        \"or remove the --do_eval argument.\"\n",
        "    )\n",
        "\n",
        "if (\n",
        "    os.path.exists(args.out_dir)\n",
        "    and len(os.listdir(args.out_dir)) > 1\n",
        "    and args.do_train\n",
        "    and not args.overwrite_out_dir\n",
        "    and not args.continue_training\n",
        "):\n",
        "    raise ValueError(\n",
        "        f\"Output directory {args.out_dir} already exists and is not empty. \"\n",
        "        f\"Use --overwrite_out_dir or --continue_training.\"\n",
        "    )"
      ],
      "metadata": {
        "id": "eykMNYcinQM1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Setup device\n",
        "print(torch.cuda.is_available())\n",
        "device = torch.device(\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"cpu\"\n",
        ")"
      ],
      "metadata": {
        "id": "IALfIzUunQM2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Set seed\n",
        "set_seed(args)\n",
        "\n",
        "# Load the models\n",
        "if args.continue_training:\n",
        "    args.model_name_or_path = args.out_dir\n",
        "# Delete the current results file\n",
        "else:\n",
        "    eval_results_file = os.path.join(args.out_dir, \"eval_results.txt\")\n",
        "    if os.path.exists(eval_results_file):\n",
        "        os.remove(eval_results_file)\n",
        "\n",
        "args.device = \"cpu\"\n",
        "tokenizer, model = init_model(\n",
        "    args.model_name_or_path, device=args.device, do_lower_case=args.do_lower_case, args = args\n",
        ")\n",
        "\n",
        "args.pad_token_id = tokenizer.pad_token_id\n",
        "logger.info(f\"Pad token ID: {args.pad_token_id}\")\n",
        "args.block_size = tokenizer.max_len_single_sentence\n",
        "logger.info(f\"Training/evaluation parameters {args}\")\n",
        "\n",
        "eval_dataset = None\n",
        "if args.do_eval or args.eval_during_train:\n",
        "    eval_dataset = EncoderDecoderTextDataset(\n",
        "        tokenizer, args, file_path=args.eval_data_file, block_size=args.block_size)\n",
        "\n",
        "# Add special tokens (if loading a model before fine-tuning)\n",
        "if args.do_train and not args.continue_training:\n",
        "    special_tokens = [\"[shuffled]\", \"[orig]\", \"<eos>\"]\n",
        "    extra_specials = [f\"<S{i}>\" for i in range(args.max_output_length)]\n",
        "    special_tokens += extra_specials\n",
        "    tokenizer.pad_token = \"<pad>\"\n",
        "    tokenizer.eos_token = \"<eos>\"\n",
        "    tokenizer.add_tokens(special_tokens)\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "args.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# resize_token_embeddings for Bart doesn't work if the model is already on the device\n",
        "args.device = device\n",
        "model= DataParallel(model)\n",
        "model.to(args.device)"
      ],
      "metadata": {
        "id": "sfNBmt6vnQM2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Training\n",
        "if args.do_train:\n",
        "    train_dataset = EncoderDecoderTextDataset(\n",
        "        tokenizer,\n",
        "        args,\n",
        "        file_path=args.train_file,\n",
        "        block_size=args.block_size,\n",
        "    )\n",
        "    global_step, tr_loss = train(\n",
        "        args,\n",
        "        train_dataset,\n",
        "        model,\n",
        "        tokenizer,\n",
        "        loss_fnc=get_loss,\n",
        "        eval_dataset=eval_dataset,\n",
        "    )\n",
        "    logger.info(f\" global_step = {global_step}, average loss = {tr_loss}\")\n",
        "\n",
        "    # Create output directory if needed\n",
        "    if not os.path.exists(args.out_dir):\n",
        "        os.makedirs(args.out_dir)\n",
        "\n",
        "    logger.info(f\"Saving model checkpoint to {args.out_dir}\")\n",
        "\n",
        "    # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "    # They can then be reloaded using `from_pretrained()`\n",
        "    model_to_save = model.module if hasattr(model, \"module\") else model\n",
        "    model_to_save.save_pretrained(args.out_dir)\n",
        "    tokenizer.save_pretrained(args.out_dir)\n",
        "\n",
        "    # Good practice: save your training arguments together with the trained model\n",
        "    torch.save(args, os.path.join(args.out_dir, \"training_args.bin\"))\n",
        "\n",
        "    # Load a trained model and vocabulary that you have fine-tuned\n",
        "    tokenizer, model = init_model(\n",
        "        args.out_dir, device=args.device, do_lower_case=args.do_lower_case, args=args\n",
        "    )\n",
        "    args.block_size = tokenizer.max_len_single_sentence\n",
        "    model= DataParallel(model)\n",
        "    model.to(args.device)"
      ],
      "metadata": {
        "id": "0H-KmOionQM2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Evaluation\n",
        "results = {}\n",
        "if args.do_eval:\n",
        "    checkpoint = args.out_dir\n",
        "    logger.info(f\"Evaluate the following checkpoint: {checkpoint}\")\n",
        "    prefix = (\n",
        "        checkpoint.split(\"/\")[-1] if checkpoint.find(\"checkpoint\") != -1 else \"\"\n",
        "    )\n",
        "    _, model = init_model(\n",
        "        checkpoint, device=args.device, do_lower_case=args.do_lower_case, args=args\n",
        "    )\n",
        "\n",
        "    model.to(args.device)\n",
        "    result = evaluate(eval_dataset, args, model, prefix=prefix, loss_fnc=get_loss)\n",
        "    results.update(result)\n",
        "print(results)"
      ],
      "metadata": {
        "id": "jwDaztM8nQM3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def generate_conditional(tokenizer, model, args, input, device):\n",
        "    \"\"\"\n",
        "    Generate a sequence with models like Bart and T5\n",
        "    \"\"\"\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(input))\n",
        "    decoder_start_token_id = input_ids[-1]\n",
        "    input_ids = torch.tensor([input_ids]).to(device)\n",
        "    max_length = args.max_length\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        do_sample=args.beams == 0,\n",
        "        max_length=max_length,\n",
        "        min_length=5,\n",
        "        temperature=args.temperature,\n",
        "        top_p=args.p if args.p > 0 else None,\n",
        "        top_k=args.k if args.k > 0 else None,\n",
        "        num_beams=args.beams if args.beams > 0 else None,\n",
        "        early_stopping=True,\n",
        "        no_repeat_ngram_size=2,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        decoder_start_token_id=decoder_start_token_id,\n",
        "        num_return_sequences=1  # max(1, args.beams)\n",
        "    )\n",
        "\n",
        "    preds = [tokenizer.decode(\n",
        "        output, skip_special_tokens=False, clean_up_tokenization_spaces=False) for output in outputs]\n",
        "\n",
        "    return preds"
      ],
      "metadata": {
        "id": "MM4WcnM0KKnd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "import tqdm\n",
        "\n",
        "\"\"\"\n",
        "Generate outputs\n",
        "\"\"\"\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "# Required\n",
        "parser.add_argument(\n",
        "    \"--in_file\",\n",
        "    default=\"/content/dataset.json\",\n",
        "    type=str,\n",
        "    help=\"The input json file\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--out_file\",\n",
        "    default=\"/content/test.json\",\n",
        "    type=str,\n",
        "    help=\"out jsonl file\",\n",
        ")\n",
        "# Optional\n",
        "parser.add_argument(\n",
        "    \"--max_length\", default=1024, type=int, required=False, help=\"Maximum text length\"\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--k\", default=0, type=int, required=False, help=\"k for top k sampling\"\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--p\", default=0, type=float, required=False, help=\"p for nucleus sampling\"\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--beams\", default=1, type=int, required=False, help=\"beams for beam search\"\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--temperature\",\n",
        "    default=1.0,\n",
        "    type=float,\n",
        "    required=False,\n",
        "    help=\"temperature for sampling\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--dataset_type\",\n",
        "    default=\"train\",\n",
        "    type=str,\n",
        "    help=\"Which part of the dataset to load.\",\n",
        ")\n",
        "\n",
        "args, unknown = parser.parse_known_args()\n",
        "\n",
        "if (\n",
        "        (args.k == args.p == args.beams == 0)\n",
        "        or (args.k != 0 and args.p != 0)\n",
        "        or (args.beams != 0 and args.p != 0)\n",
        "        or (args.beams != 0 and args.k != 0)\n",
        "):\n",
        "    raise ValueError(\n",
        "        \"Exactly one of p, k, and beams should be set to a non-zero value.\"\n",
        "    )\n",
        "\n",
        "examples = combine_data(args.in_file, args.dataset_type)\n",
        "\n",
        "logger.info(examples[:5])\n",
        "\n",
        "special_tokens = [\"[shuffled]\", \"[orig]\", \"<eos>\"]\n",
        "extra_specials = [f\"<S{i}>\" for i in range(args.max_length)]\n",
        "special_tokens += extra_specials\n",
        "\n",
        "with open(args.out_file, \"w\") as f_out:\n",
        "    for input_lines in tqdm.tqdm(examples):\n",
        "        try:\n",
        "            preds = generate_conditional(\n",
        "                tokenizer,\n",
        "                model,\n",
        "                args,\n",
        "                input_lines,\n",
        "                device,\n",
        "            )\n",
        "\n",
        "            # Remove any word that has \"]\" or \"[\" in it\n",
        "            preds = [re.sub(r\"(\\w*\\])\", \"\", pred) for pred in preds]\n",
        "            preds = [re.sub(r\"(\\[\\w*)\", \"\", pred) for pred in preds]\n",
        "            preds = [re.sub(\" +\", \" \", pred).strip() for pred in preds]\n",
        "\n",
        "        except Exception as exp:\n",
        "            logger.info(exp)\n",
        "            preds = []\n",
        "\n",
        "        f_out.write(\n",
        "            json.dumps({\"input\": input_lines, \"predictions\": preds})\n",
        "            + \"\\n\"\n",
        "        )"
      ],
      "metadata": {
        "id": "qbuBOdLTKKnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub(\"junyinc/LING575-WIN21-Reorder\")\n",
        "tokenizer.push_to_hub(\"junyinc/LING575-WIN21-Reorder\")"
      ],
      "metadata": {
        "id": "ePQ01LImFlz_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}